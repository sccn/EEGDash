{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEGDash Example for Auditory Oddball Classification\n",
    "\n",
    "This tutorial demonstrates using the *EEGDash* library with PyTorch to classify EEG responses in an auditory oddball paradigm.\n",
    "\n",
    "1. **Data Description**: Dataset contains EEG recordings during an auditory oddball task with two stimulus types:\n",
    "   - Standard: 500 Hz tone\n",
    "   - Oddball: 1000 Hz tone\n",
    "\n",
    "2. **Data Preprocessing**: \n",
    "   - Applies bandpass filtering (1-55 Hz)\n",
    "   - Selects all 64 EEG channels\n",
    "   - Creates event-based windows\n",
    "   - Processes data in batches for memory efficiency\n",
    "\n",
    "3. **Dataset Preparation**: \n",
    "   - Remaps events into two classes: oddball, standard\n",
    "   - Splits into training (80%) and test (20%) sets\n",
    "   - Creates PyTorch DataLoaders\n",
    "\n",
    "4. **Model**: \n",
    "   - ShallowFBCSPNet architecture\n",
    "   - 64 input channels, 2 output classes\n",
    "   - 256-sample input windows\n",
    "\n",
    "5. **Training**: \n",
    "   - Adamax optimizer with learning rate decay\n",
    "   - 5 training epochs\n",
    "   - Reports accuracy on train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval Using EEGDash\n",
    "\n",
    "Data retrieved from https://nemar.org/dataexplorer/detail?dataset_id=ds003061.\n",
    "\n",
    "Download locally and change the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eegdash.data_utils import EEGBIDSDataset\n",
    "\n",
    "dataset = EEGBIDSDataset(\n",
    "    data_dir=\"d:/Users/vivian/Desktop/UCSD/EEG/ds003061/ds003061\", dataset=\"ds003061\"\n",
    ")\n",
    "\n",
    "all_files = dataset.get_files()\n",
    "test_files = all_files[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Using Braindecode\n",
    "\n",
    "[Braindecode](https://braindecode.org/) provides a powerful framework for EEG data preprocessing and analysis.\n",
    "\n",
    "We apply three preprocessing steps in Braindecode:\n",
    "\n",
    "1.**Event Remapping** using event markers to convert:\n",
    "  - 3,4 → oddball (0)\n",
    "  - 6,7 → standard (1)\n",
    "\n",
    "2.**Channel Selection & Filtering**:\n",
    "  - Selecting first 64 EEG channels\n",
    "  - Bandpass filtering between 1 Hz and 55 Hz\n",
    "\n",
    "When calling the **preprocess** function, the data is retrieved from the files.\n",
    "\n",
    "Finally, we use **create_windows_from_events** to extract windows centered on events (-128 to +128 samples around each event)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All files processed, total number of windows: 1494\n",
      "Window shape: (64, 256)\n"
     ]
    }
   ],
   "source": [
    "from braindecode.preprocessing import (\n",
    "    preprocess,\n",
    "    Preprocessor,\n",
    "    create_windows_from_events,\n",
    ")\n",
    "import mne\n",
    "from mne.io import read_raw_eeglab\n",
    "from braindecode.datasets import BaseConcatDataset, BaseDataset\n",
    "import numpy as np\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "mne.set_log_level(\"ERROR\")\n",
    "logging.getLogger(\"joblib\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class OddballPreprocessor(Preprocessor):\n",
    "    def __init__(self):\n",
    "        super().__init__(fn=self.transform, apply_on_array=False)\n",
    "\n",
    "    def transform(self, raw):\n",
    "        # Get events and event dictionary\n",
    "        events, _ = mne.events_from_annotations(raw)\n",
    "\n",
    "        # Remove last event to avoid time duration issues\n",
    "        events = events[:-1]\n",
    "\n",
    "        # Map events using boolean indexing\n",
    "        oddball_mask = np.isin(events[:, 2], [3, 4])\n",
    "        standard_mask = np.isin(events[:, 2], [6, 7])\n",
    "\n",
    "        # Create new events array using array operations\n",
    "        new_events = np.zeros_like(events)\n",
    "        valid_mask = oddball_mask | standard_mask\n",
    "        new_events[valid_mask, 0] = events[valid_mask, 0]\n",
    "        new_events[standard_mask, 2] = 1  # standard events -> 1\n",
    "\n",
    "        # Filter out invalid events\n",
    "        new_events = new_events[valid_mask]\n",
    "\n",
    "        # Create annotations from events\n",
    "        annot_from_events = mne.annotations_from_events(\n",
    "            events=new_events,\n",
    "            event_desc={0: \"oddball\", 1: \"standard\"},\n",
    "            sfreq=raw.info[\"sfreq\"],\n",
    "        )\n",
    "        raw.set_annotations(annot_from_events)\n",
    "        return raw\n",
    "\n",
    "\n",
    "# Create dataset from all files\n",
    "all_datasets = [\n",
    "    BaseDataset(read_raw_eeglab(f, preload=False), target_name=None) for f in test_files\n",
    "]\n",
    "dataset_concat = BaseConcatDataset(all_datasets)\n",
    "\n",
    "# BrainDecode preprocessors\n",
    "preprocessors = [\n",
    "    OddballPreprocessor(),\n",
    "    Preprocessor(\n",
    "        \"pick_channels\",\n",
    "        ch_names=read_raw_eeglab(test_files[0], preload=False).ch_names[:64],\n",
    "    ),\n",
    "    Preprocessor(\"resample\", sfreq=128),\n",
    "    Preprocessor(\"filter\", l_freq=1, h_freq=55),\n",
    "]\n",
    "preprocess(dataset_concat, preprocessors)\n",
    "\n",
    "# Extract windows\n",
    "windows_ds = create_windows_from_events(\n",
    "    dataset_concat,\n",
    "    trial_start_offset_samples=-128,\n",
    "    trial_stop_offset_samples=128,\n",
    "    preload=False,\n",
    ")\n",
    "\n",
    "print(f\"\\nAll files processed, total number of windows: {len(windows_ds)}\")\n",
    "print(f\"Window shape: {windows_ds[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and test sets\n",
    "\n",
    "The data preparation pipeline consists of these key steps:\n",
    "\n",
    "1. **Dataset Creation** - The processed windows are automatically labeled (0=oddball, 1=standard) by the OddballPreprocessor using efficient array operations.\n",
    "\n",
    "2. **Train-Test Split** - Using sklearn's train_test_split with 80-20 split and stratified sampling.\n",
    "\n",
    "3. **PyTorch Data Preparation** - Converting to tensors and creating DataLoader objects for mini-batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1494\n",
      "Data shape: (1494, 64, 256)\n",
      "Distribution of labels: (array([0, 1]), array([ 335, 1159]))\n",
      "Label meanings: 0=oddball, 1=standard\n",
      "\n",
      "Dataset size:\n",
      "Training set: torch.Size([1195, 64, 256]), labels: torch.Size([1195])\n",
      "Test set: torch.Size([299, 64, 256]), labels: torch.Size([299])\n",
      "\n",
      "Proportion of samples of each class in training set:\n",
      "Category 0: 0.224\n",
      "Category 1: 0.776\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# Extract data and labels using array operations\n",
    "data = np.stack([windows_ds[i][0] for i in range(len(windows_ds))])\n",
    "labels = np.array([windows_ds[i][1] for i in range(len(windows_ds))])\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Dataset size: {len(data)}\")\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(\"Distribution of labels:\", np.unique(labels, return_counts=True))\n",
    "print(\"Label meanings: 0=oddball, 1=standard\")\n",
    "\n",
    "# Split into train and test sets\n",
    "train_indices, test_indices = train_test_split(\n",
    "    range(len(data)), test_size=0.2, stratify=labels, random_state=random_state\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.FloatTensor(data[train_indices])\n",
    "X_test = torch.FloatTensor(data[test_indices])\n",
    "y_train = torch.LongTensor(labels[train_indices])\n",
    "y_test = torch.LongTensor(labels[test_indices])\n",
    "\n",
    "# Create data loaders\n",
    "dataset_train = TensorDataset(X_train, y_train)\n",
    "dataset_test = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(dataset_train, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"\\nDataset size:\")\n",
    "print(f\"Training set: {X_train.shape}, labels: {y_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, labels: {y_test.shape}\")\n",
    "print(f\"\\nProportion of samples of each class in training set:\")\n",
    "for label in np.unique(labels):\n",
    "    ratio = np.mean(y_train.numpy() == label)\n",
    "    print(f\"Category {label}: {ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model\n",
    "\n",
    "The model is a shallow convolutional neural network (ShallowFBCSPNet) with 64 input channels (EEG channels), 2 output classes (oddball, standard), and an input window size of 256 samples (1 seconds of EEG data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ShallowFBCSPNet                          [1, 2]                    --\n",
       "├─Ensure4d: 1-1                          [1, 64, 256, 1]           --\n",
       "├─Rearrange: 1-2                         [1, 1, 256, 64]           --\n",
       "├─CombinedConv: 1-3                      [1, 40, 232, 1]           103,440\n",
       "├─BatchNorm2d: 1-4                       [1, 40, 232, 1]           80\n",
       "├─Expression: 1-5                        [1, 40, 232, 1]           --\n",
       "├─AvgPool2d: 1-6                         [1, 40, 11, 1]            --\n",
       "├─Expression: 1-7                        [1, 40, 11, 1]            --\n",
       "├─Dropout: 1-8                           [1, 40, 11, 1]            --\n",
       "├─Sequential: 1-9                        [1, 2]                    --\n",
       "│    └─Conv2d: 2-1                       [1, 2, 1, 1]              882\n",
       "│    └─LogSoftmax: 2-2                   [1, 2, 1, 1]              --\n",
       "│    └─Expression: 2-3                   [1, 2]                    --\n",
       "==========================================================================================\n",
       "Total params: 104,402\n",
       "Trainable params: 104,402\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.07\n",
       "Forward/backward pass size (MB): 0.07\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.14\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from braindecode.models import ShallowFBCSPNet\n",
    "from torchinfo import summary\n",
    "\n",
    "model = ShallowFBCSPNet(\n",
    "    in_chans=64, n_classes=2, input_window_samples=256, final_conv_length=\"auto\"\n",
    ")\n",
    "\n",
    "summary(model, input_size=(1, 64, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation Process\n",
    "\n",
    "The training and evaluation pipeline runs for 5 epochs using Adamax optimization. Key components include:\n",
    "\n",
    "1. **Hardware Setup** - Model allocation to CPU/GPU for optimal computation.\n",
    "\n",
    "2. **Data Processing** - Channel-wise normalization of input data using mean and standard deviation.\n",
    "\n",
    "3. **Training Process** - Each epoch performs forward passes, computes cross-entropy loss, updates parameters, and tracks accuracy.\n",
    "\n",
    "4. **Evaluation** - Model performance is assessed on the test set after each training epoch.\n",
    "\n",
    "The process monitors both training and test accuracy to track model learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start training...\n",
      "epoch 1, training accuracy: 0.794, test accuracy: 0.893\n",
      "epoch 2, training accuracy: 0.895, test accuracy: 0.923\n",
      "epoch 3, training accuracy: 0.921, test accuracy: 0.916\n",
      "epoch 4, training accuracy: 0.936, test accuracy: 0.933\n",
      "epoch 5, training accuracy: 0.945, test accuracy: 0.930\n"
     ]
    }
   ],
   "source": [
    "# Set up device, optimizer, and learning rate scheduler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=0.001, weight_decay=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1)\n",
    "\n",
    "\n",
    "def normalize_data(x):\n",
    "    mean = x.mean(dim=2, keepdim=True)\n",
    "    std = x.std(dim=2, keepdim=True) + 1e-7\n",
    "    x = (x - mean) / std\n",
    "    x = x.to(device=device, dtype=torch.float32)\n",
    "    return x\n",
    "\n",
    "\n",
    "print(\"\\nStart training...\")\n",
    "epochs = 5\n",
    "\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    correct_train = 0\n",
    "    for t, (x, y) in enumerate(train_loader):\n",
    "        scores = model(normalize_data(x))\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "        _, preds = scores.max(1)\n",
    "        correct_train += (preds == y).sum() / len(dataset_train)\n",
    "\n",
    "        loss = F.cross_entropy(scores, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    with torch.no_grad():\n",
    "        for t, (x, y) in enumerate(test_loader):\n",
    "            scores = model(normalize_data(x))\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            _, preds = scores.max(1)\n",
    "            correct_test += (preds == y).sum() / len(dataset_test)\n",
    "\n",
    "    print(\n",
    "        f\"epoch {e + 1}, training accuracy: {correct_train:.3f}, test accuracy: {correct_test:.3f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eegtemp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
