{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab:\n# `pip install eegdash`\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Challenge 1: Cross-Task Transfer Learning!\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/eeg2025/startkit/blob/main/challenge_1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Challenge 1: Cross-Task Transfer Learning!\n\n## How can we use the knowledge from one EEG Decoding task into another?\n\nTransfer learning is a widespread technique used in deep learning. It uses knowledge learned from one source task/domain in another target task/domain. It has been studied in depth in computer vision, natural language processing, and speech, but what about EEG brain decoding?\n\nThe cross-task transfer learning scenario in EEG decoding is remarkably underexplored in comparison to the developers of new models, [Aristimunha et al., (2023)](https://arxiv.org/abs/2308.02408), even though it can be much more useful for real applications, see [Wimpff et al. (2025)](https://arxiv.org/abs/2502.06828), [Wu et al. (2025)](https://arxiv.org/abs/2507.09882).\n\nOur Challenge 1 addresses a key goal in neurotechnology: decoding cognitive function from EEG using the pre-trained knowledge from another. In other words, developing models that can effectively transfer/adapt/adjust/fine-tune knowledge from passive EEG tasks to active tasks.\n\nThe ability to generalize and transfer is something critical that we believe should be focused. To go beyond just comparing metrics numbers that are often not comparable, given the specificities of EEG, such as pre-processing, inter-subject variability, and many other unique components of this type of data.\n\nThis means your submitted model might be trained on a subset of tasks and fine-tuned on data from another condition, evaluating its capacity to generalize with task-specific fine-tuning.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__________\n\nNote: For simplicity purposes, we will only show how to do the decoding directly in our target task, and it is up to the teams to think about how to use the passive task to perform the pre-training.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Summary table for this start kit\n\nIn this tutorial, we are going to show in more detail what we want from Challenge 1:\n\n**Contents**:\n\n0. Understand the Contrast Change Detection - CCD task.\n1. Understand the [`EEGChallengeDataset`](https://eeglab.org/EEGDash/api/eegdash.html#eegdash.EEGChallengeDataset) object.\n2. Preparing the dataloaders.\n3. Building the deep learning model with [`braindecode`](https://braindecode.org/stable/models/models_table.html).\n4. Designing the training loop.\n5. Training the model.\n6. Evaluating test performance.\n7. Going further, *benchmark go brrr!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!pip install braindecode\n!pip install eegdash\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and setup\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\nimport torch\nfrom braindecode.datasets import BaseConcatDataset\nfrom braindecode.preprocessing import (\n    preprocess,\n    Preprocessor,\n    create_windows_from_events,\n)\nfrom braindecode.models import EEGNeX\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import check_random_state\nfrom typing import Optional\nfrom torch.nn import Module\nfrom torch.optim.lr_scheduler import LRScheduler\nfrom tqdm import tqdm\nimport copy\nfrom joblib import Parallel, delayed\n\n# Identify whether a CUDA-enabled GPU is available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif device == \"cuda\":\n    msg = \"CUDA-enabled GPU found. Training should be faster.\"\nelse:\n    msg = (\n        \"No GPU found. Training will be carried out on CPU, which might be \"\n        \"slower.\\n\\nIf running on Google Colab, you can request a GPU runtime by\"\n        \" clicking\\n`Runtime/Change runtime type` in the top bar menu, then \"\n        \"selecting 'T4 GPU'\\nunder 'Hardware accelerator'.\"\n    )\nprint(msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What are we decoding?\n\nThe Contrast Change Detection (CCD) task relates to [Steady-State Visual Evoked Potentials (SSVEP)](https://en.wikipedia.org/wiki/Steady-state_visually_evoked_potential) and [Event-Related Potentials (ERP)](https://en.wikipedia.org/wiki/Event-related_potential).\n\nAlgorithmically, what the subject sees during recording is:\n\n* Two flickering striped discs: one tilted left, one tilted right.\n* After a variable delay, **one disc's contrast gradually increases** **while the other decreases**.\n* They **press left or right** to indicate which disc got stronger.\n* They receive **feedback** (\ud83d\ude42 correct / \ud83d\ude41 incorrect).\n\n**The task parallels SSVEP and ERP:**\n\n* The continuous flicker **tags the EEG at fixed frequencies (and harmonics)** \u2192 SSVEP-like signals.\n* The **ramp onset**, the **button press**, and the **feedback** are **time-locked events** that yield ERP-like components.\n\nYour task (**label**) is to predict the response time for the subject during this windows.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Load the competition dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from eegdash.dataset import EEGChallengeDataset\nfrom eegdash.hbn.windows import (\n    annotate_trials_with_target,\n    add_aux_anchors,\n    keep_only_recordings_with,\n    add_extras_columns,\n)\n\nDATA_DIR = Path(\"data\")\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\ndataset_ccd = EEGChallengeDataset(\n    task=\"contrastChangeDetection\", release=\"R5\", cache_dir=DATA_DIR, mini=True\n)\n\n# For visualization purposes, we will see just one object.\nraw = dataset_ccd.datasets[0].raw  # get the Raw object of the first recording\n\n# To download all data directly, you can do:\nraws = Parallel(n_jobs=-1)(delayed(lambda d: d.raw)(d) for d in dataset_ccd.datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Create windows of interest\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "EPOCH_LEN_S = 2.0\nSFREQ = 100  # by definition here\n\ntransformation_offline = [\n    Preprocessor(\n        annotate_trials_with_target,\n        target_field=\"rt_from_stimulus\",\n        epoch_length=EPOCH_LEN_S,\n        require_stimulus=True,\n        require_response=True,\n        apply_on_array=False,\n    ),\n    Preprocessor(add_aux_anchors, apply_on_array=False),\n]\npreprocess(dataset_ccd, transformation_offline, n_jobs=1)\n\nANCHOR = \"stimulus_anchor\"\nSHIFT_AFTER_STIM = 0.5\nWINDOW_LEN = 2.0\n\n# Keep only recordings that actually contain stimulus anchors\ndataset = keep_only_recordings_with(ANCHOR, dataset_ccd)\n\n# Create single-interval windows (stim-locked, long enough to include the response)\nsingle_windows = create_windows_from_events(\n    dataset,\n    mapping={ANCHOR: 0},\n    trial_start_offset_samples=int(SHIFT_AFTER_STIM * SFREQ),  # +0.5 s\n    trial_stop_offset_samples=int((SHIFT_AFTER_STIM + WINDOW_LEN) * SFREQ),  # +2.5 s\n    window_size_samples=int(EPOCH_LEN_S * SFREQ),\n    window_stride_samples=SFREQ,\n    preload=True,\n)\n\n# Injecting metadata into the extra mne annotation.\nsingle_windows = add_extras_columns(\n    single_windows,\n    dataset,\n    desc=ANCHOR,\n    keys=(\n        \"target\",\n        \"rt_from_stimulus\",\n        \"rt_from_trialstart\",\n        \"stimulus_onset\",\n        \"response_onset\",\n        \"correct\",\n        \"response_type\",\n    ),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Split the data\nExtract meta information\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "meta_information = single_windows.get_metadata()\n\nvalid_frac = 0.1\ntest_frac = 0.1\nseed = 2025\n\nsubjects = meta_information[\"subject\"].unique()\n\ntrain_subj, valid_test_subject = train_test_split(\n    subjects,\n    test_size=(valid_frac + test_frac),\n    random_state=check_random_state(seed),\n    shuffle=True,\n)\n\nvalid_subj, test_subj = train_test_split(\n    valid_test_subject,\n    test_size=test_frac,\n    random_state=check_random_state(seed + 1),\n    shuffle=True,\n)\n\n# Sanity check\nassert (set(valid_subj) | set(test_subj) | set(train_subj)) == set(subjects)\n\n# Create train/valid/test splits for the windows\nsubject_split = single_windows.split(\"subject\")\ntrain_set = []\nvalid_set = []\ntest_set = []\n\nfor s in subject_split:\n    if s in train_subj:\n        train_set.append(subject_split[s])\n    elif s in valid_subj:\n        valid_set.append(subject_split[s])\n    elif s in test_subj:\n        test_set.append(subject_split[s])\n\ntrain_set = BaseConcatDataset(train_set)\nvalid_set = BaseConcatDataset(valid_set)\ntest_set = BaseConcatDataset(test_set)\n\nprint(\"Number of examples in each split in the minirelease\")\nprint(f\"Train:\\t{len(train_set)}\")\nprint(f\"Valid:\\t{len(valid_set)}\")\nprint(f\"Test:\\t{len(test_set)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Create dataloaders\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n# Set num_workers to 0 to avoid multiprocessing issues in notebooks/tutorials\nnum_workers = 0\n\ntrain_loader = DataLoader(\n    train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers\n)\nvalid_loader = DataLoader(\n    valid_set, batch_size=batch_size, shuffle=False, num_workers=num_workers\n)\ntest_loader = DataLoader(\n    test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Build the model\nFor any braindecode model, you can initialize only inputing the signal related parameters\nYou can use any Pytorch module that you want here.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = EEGNeX(\n    n_chans=129,  # 129 channels\n    n_outputs=1,  # 1 output for regression\n    n_times=200,  # 2 seconds\n    sfreq=100,  # sample frequency 100 Hz\n)\n\nprint(model)\nmodel.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Define training and validation functions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(\n    dataloader: DataLoader,\n    model: Module,\n    loss_fn,\n    optimizer,\n    scheduler: Optional[LRScheduler],\n    epoch: int,\n    device,\n    print_batch_stats: bool = True,\n):\n    model.train()\n\n    total_loss = 0.0\n    sum_sq_err = 0.0\n    n_samples = 0\n\n    progress_bar = tqdm(\n        enumerate(dataloader), total=len(dataloader), disable=not print_batch_stats\n    )\n\n    for batch_idx, batch in progress_bar:\n        # Support datasets that may return (X, y) or (X, y, ...)\n        X, y = batch[0], batch[1]\n        X, y = X.to(device).float(), y.to(device).float()\n\n        optimizer.zero_grad(set_to_none=True)\n        preds = model(X)\n        loss = loss_fn(preds, y)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        # Flatten to 1D for regression metrics and accumulate squared error\n        preds_flat = preds.detach().view(-1)\n        y_flat = y.detach().view(-1)\n        sum_sq_err += torch.sum((preds_flat - y_flat) ** 2).item()\n        n_samples += y_flat.numel()\n\n        if print_batch_stats:\n            running_rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n            progress_bar.set_description(\n                f\"Epoch {epoch}, Batch {batch_idx + 1}/{len(dataloader)}, \"\n                f\"Loss: {loss.item():.6f}, RMSE: {running_rmse:.6f}\"\n            )\n\n    if scheduler is not None:\n        scheduler.step()\n\n    avg_loss = total_loss / len(dataloader)\n    rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n    return avg_loss, rmse\n\n\n@torch.no_grad()\ndef valid_model(\n    dataloader: DataLoader,\n    model: Module,\n    loss_fn,\n    device,\n    print_batch_stats: bool = True,\n):\n    model.eval()\n\n    total_loss = 0.0\n    sum_sq_err = 0.0\n    n_batches = len(dataloader)\n    n_samples = 0\n\n    iterator = tqdm(\n        enumerate(dataloader), total=n_batches, disable=not print_batch_stats\n    )\n\n    for batch_idx, batch in iterator:\n        # Supports (X, y) or (X, y, ...)\n        X, y = batch[0], batch[1]\n        X, y = X.to(device).float(), y.to(device).float()\n\n        preds = model(X)\n        batch_loss = loss_fn(preds, y).item()\n        total_loss += batch_loss\n\n        preds_flat = preds.detach().view(-1)\n        y_flat = y.detach().view(-1)\n        sum_sq_err += torch.sum((preds_flat - y_flat) ** 2).item()\n        n_samples += y_flat.numel()\n\n        if print_batch_stats:\n            running_rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n            iterator.set_description(\n                f\"Val Batch {batch_idx + 1}/{n_batches}, \"\n                f\"Loss: {batch_loss:.6f}, RMSE: {running_rmse:.6f}\"\n            )\n\n    avg_loss = total_loss / n_batches if n_batches else float(\"nan\")\n    rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n\n    print(f\"Val RMSE: {rmse:.6f}, Val Loss: {avg_loss:.6f}\\n\")\n    return avg_loss, rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Train the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lr = 1e-3\nweight_decay = 1e-5\nn_epochs = (\n    5  # For demonstration purposes, we use just 5 epochs here. You can increase this.\n)\nearly_stopping_patience = 50\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs - 1)\nloss_fn = torch.nn.MSELoss()\n\npatience = 5\nmin_delta = 1e-4\nbest_rmse = float(\"inf\")\nepochs_no_improve = 0\nbest_state, best_epoch = None, None\n\nfor epoch in range(1, n_epochs + 1):\n    print(f\"Epoch {epoch}/{n_epochs}: \", end=\"\")\n\n    train_loss, train_rmse = train_one_epoch(\n        train_loader, model, loss_fn, optimizer, scheduler, epoch, device\n    )\n    val_loss, val_rmse = valid_model(test_loader, model, loss_fn, device)\n\n    print(\n        f\"Train RMSE: {train_rmse:.6f}, \"\n        f\"Average Train Loss: {train_loss:.6f}, \"\n        f\"Val RMSE: {val_rmse:.6f}, \"\n        f\"Average Val Loss: {val_loss:.6f}\"\n    )\n\n    if val_rmse < best_rmse - min_delta:\n        best_rmse = val_rmse\n        best_state = copy.deepcopy(model.state_dict())\n        best_epoch = epoch\n        epochs_no_improve = 0\n    else:\n        epochs_no_improve += 1\n        if epochs_no_improve >= patience:\n            print(\n                f\"Early stopping at epoch {epoch}. Best Val RMSE: {best_rmse:.6f} (epoch {best_epoch})\"\n            )\n            break\n\nif best_state is not None:\n    model.load_state_dict(best_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Save the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"weights_challenge_1.pt\")\nprint(\"Model saved as 'weights_challenge_1.pt'\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}