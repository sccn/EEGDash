{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab:\n# `pip install eegdash`\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n.. meta::\n   :html_theme.sidebar_secondary.remove: true\n\n# Challenge 1: Cross-Task Transfer Learning!\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" target=\"https://colab.research.google.com/github/eeg2025/startkit/blob/main/challenge_1.ipynb\" alt=\"Open In Colab\">\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preliminary notes\nBefore we begin, I just want to make a deal with you, ok?\nThis is a community competition with a strong open-source foundation.\nWhen I say open-source, I mean volunteer work.\n\nSo, if you see something that does not work or could be improved, first, **please be kind**, and\nwe will fix it together on GitHub, okay?\n\nThe entire decoding community will only go further when we stop\nsolving the same problems over and over again, and it starts working together.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How can we use the knowledge from one EEG Decoding task into another?\nTransfer learning is a widespread technique used in deep learning. It\nuses knowledge learned from one source task/domain in another target\ntask/domain. It has been studied in depth in computer vision, natural\nlanguage processing, and speech, but what about EEG brain decoding?\n\nThe cross-task transfer learning scenario in EEG decoding is remarkably\nunderexplored compared to the development of new models,\n[Aristimunha et al. (2023)](https://arxiv.org/abs/2308.02408)_, even\nthough it can be much more useful for real applications, see\n[Wimpff et al. (2025)](https://arxiv.org/abs/2502.06828)_,\n[Wu et al. (2025)](https://arxiv.org/abs/2507.09882)_.\n\nOur Challenge 1 addresses a key goal in neurotechnology: decoding\ncognitive function from EEG using the pre-trained knowledge from another.\nIn other words, developing models that can effectively\ntransfer/adapt/adjust/fine-tune knowledge from passive EEG tasks to\nactive tasks.\n\nThe ability to generalize and transfer is something critical that we\nbelieve should be focused on. To go beyond just comparing metrics numbers\nthat are often not comparable, given the specificities of EEG, such as\npre-processing, inter-subject variability, and many other unique\ncomponents of this type of data.\n\nThis means your submitted model might be trained on a subset of tasks\nand fine-tuned on data from another condition, evaluating its capacity to\ngeneralize with task-specific fine-tuning.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__________\n\nNote: For simplicity purposes, we will only show how to do the decoding\ndirectly in our target task, and it is up to the teams to think about\nhow to use the passive task to perform the pre-training.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install dependencies\nFor the challenge, we will need two significant dependencies:\n`braindecode` and `eegdash`. The libraries will install PyTorch,\nPytorch Audio, Scikit-learn, MNE, MNE-BIDS, and many other packages\nnecessary for the many functions.\n\nInstall dependencies on colab or your local machine, as eegdash\nhave braindecode as a dependency.\nyou can just run ``pip install eegdash``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and setup\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\nimport torch\nfrom braindecode.datasets import BaseConcatDataset\nfrom braindecode.preprocessing import (\n    preprocess,\n    Preprocessor,\n    create_windows_from_events,\n)\nfrom braindecode.models import EEGNeX\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import check_random_state\nfrom typing import Optional\nfrom torch.nn import Module\nfrom torch.optim.lr_scheduler import LRScheduler\nfrom tqdm import tqdm\nimport copy\nfrom joblib import Parallel, delayed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check GPU availability\n\nIdentify whether a CUDA-enabled GPU is available\nand set the device accordingly.\nIf using Google Colab, ensure that the runtime is set to use a GPU.\nThis can be done by navigating to `Runtime` > `Change runtime type` and selecting\n`GPU` as the hardware accelerator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif device == \"cuda\":\n    msg = \"CUDA-enabled GPU found. Training should be faster.\"\nelse:\n    msg = (\n        \"No GPU found. Training will be carried out on CPU, which might be \"\n        \"slower.\\n\\nIf running on Google Colab, you can request a GPU runtime by\"\n        \" clicking\\n`Runtime/Change runtime type` in the top bar menu, then \"\n        \"selecting 'T4 GPU'\\nunder 'Hardware accelerator'.\"\n    )\nprint(msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What are we decoding?\n ---------------------\n\n To start to talk about what we want to analyse, the important thing\n is to understand some basic concepts.\n\n#####################################################################\n The brain decodes the problem\n -----------------------------\n\n Broadly speaking, here *brain decoding* is the following problem:\n given brain time-series signals $X \\in \\mathbb{R}^{C \\times T}$ with\n labels $y \\in \\mathcal{Y}$, we implement a neural network $f$ that\n **decodes/translates** brain activity into the target label.\n\n We aim to translate recorded brain activity into its originating\n stimulus, behavior, or mental state, [King, J-R. et al. (2020)](https://lauragwilliams.github.io/d/m/CognitionAlgorithm.pdf)_.\n\n The neural network $f$ applies a series of transformation layers\n (e.g., ``torch.nn.Conv2d``, ``torch.nn.Linear``, ``torch.nn.ELU``, ``torch.nn.BatchNorm2d``)\n to the data to filter, extract features, and learn embeddings\n relevant to the optimization objective\u2014in other words:\n\n .. math::\n\n    f_{\\theta}: X \\to y,\n\n where $C$ (``n_chans``) is the number of channels/electrodes and $T$ (``n_times``)\n is the temporal window length/epoch size over the interval of interest.\n Here, $\\theta$ denotes the parameters learned by the neural network.\n\n Input/Output definition\n ---------------------------\n For the competition, the HBN-EEG (Healthy Brain Network EEG Datasets)\n dataset has ``n_chans = 129`` with the last channels as a [reference channel](https://mne.tools/stable/auto_tutorials/preprocessing/55_setting_eeg_reference.html),\n and we define the window length as ``n_times = 200``, corresponding to 2-second windows.\n\n Your model should follow this definition exactly; any specific selection of channels,\n filtering, or domain-adaptation technique must be performed **within the layers of the neural network model**.\n\n In this tutorial, we will use the ``EEGNeX`` model from ``braindecode`` as an example.\n You can use any model you want, as long as it follows the input/output\n definitions above.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understand the task: Contrast Change Detection (CCD)\n --------------------------------------------------------\n If you are interested to get more neuroscience insight, we recommend these two references, [HBN-EEG](https://www.biorxiv.org/content/10.1101/2024.10.03.615261v2.full.pdf)_ and [Langer, N et al. (2017)](https://www.nature.com/articles/sdata201740#Sec2)_.\n Your task (**label**) is to predict the response time for the subject during this windows.\n\n In the Video, we have an example of recording cognitive activity:\n\n The Contrast Change Detection (CCD) task relates to\n [Steady-State Visual Evoked Potentials (SSVEP)](https://en.wikipedia.org/wiki/Steady-state_visually_evoked_potential)_\n and [Event-Related Potentials (ERP)](https://en.wikipedia.org/wiki/Event-related_potential)_.\n\n Algorithmically, what the subject sees during recording is:\n\n * Two flickering striped discs: one tilted left, one tilted right.\n * After a variable delay, **one disc's contrast gradually increases** **while the other decreases**.\n * They **press left or right** to indicate which disc got stronger.\n * They receive **feedback** (\ud83d\ude42 correct / \ud83d\ude41 incorrect).\n\n **The task parallels SSVEP and ERP:**\n\n * The continuous flicker **tags the EEG at fixed frequencies (and harmonics)** \u2192 SSVEP-like signals.\n * The **ramp onset**, the **button press**, and the **feedback** are **time-locked events** that yield ERP-like components.\n\n Your task (**label**) is to predict the response time for the subject during this windows.\n\n######################################################################\n In the figure below, we have the timeline representation of the cognitive task:\n\n .. image:: https://eeg2025.github.io/assets/img/image-2.jpg\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stimulus demonstration\n ----------------------\n .. raw:: html\n\n    <div class=\"video-wrapper\">\n      <iframe src=\"https://www.youtube.com/embed/tOW2Vu2zHoU?start=1630\"\n              title=\"Contrast Change Detection (CCD) task demo\"\n              allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n              allowfullscreen></iframe>\n    </div>\n\n#####################################################################\n PyTorch Dataset for the competition\n -----------------------------------\n Now, we have a Pytorch Dataset object that contains the set of recordings for the task\n `contrastChangeDetection`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from eegdash.dataset import EEGChallengeDataset\nfrom eegdash.hbn.windows import (\n    annotate_trials_with_target,\n    add_aux_anchors,\n    keep_only_recordings_with,\n    add_extras_columns,\n)\n\n# Match tests' cache layout under ~/eegdash_cache/eeg_challenge_cache\nDATA_DIR = (Path.home() / \"eegdash_cache\" / \"eeg_challenge_cache\").resolve()\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\ndataset_ccd = EEGChallengeDataset(\n    task=\"contrastChangeDetection\", release=\"R5\", cache_dir=DATA_DIR, mini=True\n)\n# The dataset contains 20 subjects in the minirelease, and each subject has multiple recordings\n# (sessions). Each recording is represented as a dataset object within the `dataset_ccd.datasets` list.\nprint(f\"Number of recordings in the dataset: {len(dataset_ccd.datasets)}\")\nprint(\n    f\"Number of unique subjects in the dataset: {dataset_ccd.description['subject'].nunique()}\"\n)\n#\n# This dataset object have very rich Raw object details that can help you to\n# understand better the data. The framework behind this is braindecode,\n# and if you want to understand in depth what is happening, we recommend the\n# braindecode github itself.\n#\n# We can also access the Raw object for visualization purposes, we will see just one object.\nraw = dataset_ccd.datasets[0].raw  # get the Raw object of the first recording\n# And to download all the data all data directly, you can do:\nraws = Parallel(n_jobs=-1)(delayed(lambda d: d.raw)(d) for d in dataset_ccd.datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternatives for Downloading the data\n\nYou can also perform this operation with wget or the aws cli.\nThese options will probably be faster!\nPlease check more details in the `HBN` data webpage [HBN-EEG](https://neuromechanist.github.io/data/hbn/)_.\nYou need to download the 100Hz preprocessed data in BDF format.\n\nExample of wget for release R1\n   wget https://sccn.ucsd.edu/download/eeg2025/R1_L100_bdf.zip -O R1_L100_bdf.zip\n\nExample of AWS CLI for release R1\n\n   aws s3 sync s3://nmdatasets/NeurIPS25/R1_L100_bdf data/R1_L100_bdf --no-sign-request\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create windows of interest\nSo we epoch after the stimulus moment with a beginning shift of 500 ms.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "EPOCH_LEN_S = 2.0\nSFREQ = 100  # by definition here\n\ntransformation_offline = [\n    Preprocessor(\n        annotate_trials_with_target,\n        target_field=\"rt_from_stimulus\",\n        epoch_length=EPOCH_LEN_S,\n        require_stimulus=True,\n        require_response=True,\n        apply_on_array=False,\n    ),\n    Preprocessor(add_aux_anchors, apply_on_array=False),\n]\npreprocess(dataset_ccd, transformation_offline, n_jobs=1)\n\nANCHOR = \"stimulus_anchor\"\nSHIFT_AFTER_STIM = 0.5\nWINDOW_LEN = 2.0\n\n# Keep only recordings that actually contain stimulus anchors\ndataset = keep_only_recordings_with(ANCHOR, dataset_ccd)\n\n# Create single-interval windows (stim-locked, long enough to include the response)\nsingle_windows = create_windows_from_events(\n    dataset,\n    mapping={ANCHOR: 0},\n    trial_start_offset_samples=int(SHIFT_AFTER_STIM * SFREQ),  # +0.5 s\n    trial_stop_offset_samples=int((SHIFT_AFTER_STIM + WINDOW_LEN) * SFREQ),  # +2.5 s\n    window_size_samples=int(EPOCH_LEN_S * SFREQ),\n    window_stride_samples=SFREQ,\n    preload=True,\n)\n\n# Injecting metadata into the extra mne annotation.\nsingle_windows = add_extras_columns(\n    single_windows,\n    dataset,\n    desc=ANCHOR,\n    keys=(\n        \"target\",\n        \"rt_from_stimulus\",\n        \"rt_from_trialstart\",\n        \"stimulus_onset\",\n        \"response_onset\",\n        \"correct\",\n        \"response_type\",\n    ),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect the label distribution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom skorch.helper import SliceDataset\n\ny_label = np.array(list(SliceDataset(single_windows, 1)))\n\n# Plot histogram of the response times with matplotlib\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.hist(y_label, bins=30)\nax.set_title(\"Response Time Distribution\")\nax.set_xlabel(\"Response Time (s)\")\nax.set_ylabel(\"Count\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split the data\nExtract meta information\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "meta_information = single_windows.get_metadata()\n\nvalid_frac = 0.1\ntest_frac = 0.1\nseed = 2025\n\nsubjects = meta_information[\"subject\"].unique()\n\ntrain_subj, valid_test_subject = train_test_split(\n    subjects,\n    test_size=(valid_frac + test_frac),\n    random_state=check_random_state(seed),\n    shuffle=True,\n)\n\nvalid_subj, test_subj = train_test_split(\n    valid_test_subject,\n    test_size=test_frac,\n    random_state=check_random_state(seed + 1),\n    shuffle=True,\n)\n\n# Sanity check\nassert (set(valid_subj) | set(test_subj) | set(train_subj)) == set(subjects)\n\n# Create train/valid/test splits for the windows\nsubject_split = single_windows.split(\"subject\")\ntrain_set = []\nvalid_set = []\ntest_set = []\n\nfor s in subject_split:\n    if s in train_subj:\n        train_set.append(subject_split[s])\n    elif s in valid_subj:\n        valid_set.append(subject_split[s])\n    elif s in test_subj:\n        test_set.append(subject_split[s])\n\ntrain_set = BaseConcatDataset(train_set)\nvalid_set = BaseConcatDataset(valid_set)\ntest_set = BaseConcatDataset(test_set)\n\nprint(\"Number of examples in each split in the minirelease\")\nprint(f\"Train:\\t{len(train_set)}\")\nprint(f\"Valid:\\t{len(valid_set)}\")\nprint(f\"Test:\\t{len(test_set)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create dataloaders\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n# Set num_workers to 0 to avoid multiprocessing issues in notebooks/tutorials\nnum_workers = 0\n\ntrain_loader = DataLoader(\n    train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers\n)\nvalid_loader = DataLoader(\n    valid_set, batch_size=batch_size, shuffle=False, num_workers=num_workers\n)\ntest_loader = DataLoader(\n    test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the model\nFor neural network models, **to start**, we suggest using [braindecode models](https://braindecode.org/1.2/models/models_table.html)_ zoo.\nWe have implemented several different models for decoding the brain timeseries.\nYour team's responsibility is to develop a PyTorch module that receives the three-dimensional (`batch`, `n_chans`, `n_times`)\ninput and outputs the contrastive response time.\n**You can use any model you want**, as long as it follows the input/output\ndefinitions above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = EEGNeX(\n    n_chans=129,  # 129 channels\n    n_outputs=1,  # 1 output for regression\n    n_times=200,  # 2 seconds\n    sfreq=100,  # sample frequency 100 Hz\n)\n\nprint(model)\nmodel.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define training and validation functions\nThe rest is our classic PyTorch/torch lighting/skorch training pipeline,\nyou can use any training framework you want.\nWe provide a simple training and validation loop below.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(\n    dataloader: DataLoader,\n    model: Module,\n    loss_fn,\n    optimizer,\n    scheduler: Optional[LRScheduler],\n    epoch: int,\n    device,\n    print_batch_stats: bool = True,\n):\n    model.train()\n\n    total_loss = 0.0\n    sum_sq_err = 0.0\n    n_samples = 0\n\n    progress_bar = tqdm(\n        enumerate(dataloader), total=len(dataloader), disable=not print_batch_stats\n    )\n\n    for batch_idx, batch in progress_bar:\n        # Support datasets that may return (X, y) or (X, y, ...)\n        X, y = batch[0], batch[1]\n        X, y = X.to(device).float(), y.to(device).float()\n\n        optimizer.zero_grad(set_to_none=True)\n        preds = model(X)\n        loss = loss_fn(preds, y)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        # Flatten to 1D for regression metrics and accumulate squared error\n        preds_flat = preds.detach().view(-1)\n        y_flat = y.detach().view(-1)\n        sum_sq_err += torch.sum((preds_flat - y_flat) ** 2).item()\n        n_samples += y_flat.numel()\n\n        if print_batch_stats:\n            running_rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n            progress_bar.set_description(\n                f\"Epoch {epoch}, Batch {batch_idx + 1}/{len(dataloader)}, \"\n                f\"Loss: {loss.item():.6f}, RMSE: {running_rmse:.6f}\"\n            )\n\n    if scheduler is not None:\n        scheduler.step()\n\n    avg_loss = total_loss / len(dataloader)\n    rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n    return avg_loss, rmse\n\n\n@torch.no_grad()\ndef valid_model(\n    dataloader: DataLoader,\n    model: Module,\n    loss_fn,\n    device,\n    print_batch_stats: bool = True,\n):\n    model.eval()\n\n    total_loss = 0.0\n    sum_sq_err = 0.0\n    n_batches = len(dataloader)\n    n_samples = 0\n\n    iterator = tqdm(\n        enumerate(dataloader), total=n_batches, disable=not print_batch_stats\n    )\n\n    for batch_idx, batch in iterator:\n        # Supports (X, y) or (X, y, ...)\n        X, y = batch[0], batch[1]\n        X, y = X.to(device).float(), y.to(device).float()\n\n        preds = model(X)\n        batch_loss = loss_fn(preds, y).item()\n        total_loss += batch_loss\n\n        preds_flat = preds.detach().view(-1)\n        y_flat = y.detach().view(-1)\n        sum_sq_err += torch.sum((preds_flat - y_flat) ** 2).item()\n        n_samples += y_flat.numel()\n\n        if print_batch_stats:\n            running_rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n            iterator.set_description(\n                f\"Val Batch {batch_idx + 1}/{n_batches}, \"\n                f\"Loss: {batch_loss:.6f}, RMSE: {running_rmse:.6f}\"\n            )\n\n    avg_loss = total_loss / n_batches if n_batches else float(\"nan\")\n    rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n\n    print(f\"Val RMSE: {rmse:.6f}, Val Loss: {avg_loss:.6f}\\n\")\n    return avg_loss, rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lr = 1e-3\nweight_decay = 1e-5\nn_epochs = (\n    5  # For demonstration purposes, we use just 5 epochs here. You can increase this.\n)\nearly_stopping_patience = 50\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs - 1)\nloss_fn = torch.nn.MSELoss()\n\npatience = 5\nmin_delta = 1e-4\nbest_rmse = float(\"inf\")\nepochs_no_improve = 0\nbest_state, best_epoch = None, None\n\nfor epoch in range(1, n_epochs + 1):\n    print(f\"Epoch {epoch}/{n_epochs}: \", end=\"\")\n\n    train_loss, train_rmse = train_one_epoch(\n        train_loader, model, loss_fn, optimizer, scheduler, epoch, device\n    )\n    val_loss, val_rmse = valid_model(test_loader, model, loss_fn, device)\n\n    print(\n        f\"Train RMSE: {train_rmse:.6f}, \"\n        f\"Average Train Loss: {train_loss:.6f}, \"\n        f\"Val RMSE: {val_rmse:.6f}, \"\n        f\"Average Val Loss: {val_loss:.6f}\"\n    )\n\n    if val_rmse < best_rmse - min_delta:\n        best_rmse = val_rmse\n        best_state = copy.deepcopy(model.state_dict())\n        best_epoch = epoch\n        epochs_no_improve = 0\n    else:\n        epochs_no_improve += 1\n        if epochs_no_improve >= patience:\n            print(\n                f\"Early stopping at epoch {epoch}. Best Val RMSE: {best_rmse:.6f} (epoch {best_epoch})\"\n            )\n            break\n\nif best_state is not None:\n    model.load_state_dict(best_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"weights_challenge_1.pt\")\nprint(\"Model saved as 'weights_challenge_1.pt'\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}